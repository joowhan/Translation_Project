{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49d55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math as mt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from hangul_utils import split_syllables, join_jamos\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "#from eunjeon import Mecab\n",
    "from konlpy.tag import Mecab\n",
    "from hanspell import spell_checker\n",
    "from khaiii import KhaiiiApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb9ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_dict = [\n",
    "    \n",
    "    ['ㅏㅣ','ㅐ'], ['ㅑㅣ','ㅒ'], ['ㅓㅣ','ㅔ'],\n",
    "    ['ㅕㅣ','ㅖ'], ['ㅗㅣ','ㅚ'], ['ㅗㅐ','ㅙ'],\n",
    "    ['ㅜㅓ','ㅝ'], ['ㅜㅔ','ㅞ'], ['ㅡㅣ','ㅢ'],\n",
    "    ['ㅣㅏ','ㅑ'], ['ㅣㅓ','ㅕ'], ['ㅣㅗ','ㅛ'],\n",
    "    ['ㅣㅜ','ㅠ'], ['ㅡㅓ','ㅓ'], ['ㅗㅏ','ㅘ']\n",
    "    \n",
    "]\n",
    "\n",
    "jongsung_list = [ 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "#lis_beta = ['EP+EF', 'VCP+EF', 'B+EF', 'B+EP+EF', 'B+VCP+EF', 'EF','EP']\n",
    "\n",
    "lis_beta = ['EP+EF', 'EF', 'B+EF', 'B+EP+EF']\n",
    "\n",
    "#어말을 처리해 주기 위한 것으로, 나중에 EC등이 필요해 진다면 이 부분에 EC 등을 집어넣어준다. 참고로 말하자면 이는 마지막에 위치해야한다.\n",
    "#특이 바로 밑의 이 부분은 형태소 태그가 이 리스트 안의 것과 일치하는 경우 단순 삭제를 하는 것이고\n",
    "lis_beta_ef = ['EP+EP+EF', 'EP+EF', 'EF', 'UNKNOWN']\n",
    "lis_beta_ef_h = ['EF', 'UNKNOWN']\n",
    "\n",
    "#이 부분 같은 경우는 마지막에 오는 것을 처리하는 것으로 단독으로만 들어가는게 좋겠지?\n",
    "lis_tag_last = ['EF', 'UNKNOWN']\n",
    "\n",
    "lis_end = [\n",
    "    \n",
    "    'ㅂㄴㅣㄷㅏ', 'ㅂㅅㅣㄷㅏ','ㅅㅣㄷㅏ', 'ㄷㅏㅂㄴㅣㄷㅏ',\n",
    "    'ㅅㅔㅇㅛ', 'ㄷㅔㅇㅛ', 'ㅇㅔㅇㅛ', 'ㅇㅖㅇㅛ', 'ㄴㅏㅇㅛ', 'ㅇㅡㄹㄲㅏㅇㅛ', 'ㅇㅣㄹㄲㅏㅇㅛ', 'ㄹㄲㅏㅇㅛ', 'ㅇㅡㄴㄱㅏㅇㅛ', 'ㅇㅣㄴㄱㅏㅇㅛ','ㅇㅛ',\n",
    "    'ㅈㅛ',\n",
    "    'ㅅㅣㅂㅅㅣㅇㅗ', 'ㅅㅣㅇㅗ', 'ㅇㅗ',\n",
    "    'ㅂㄴㅣㄲㅏ', 'ㅅㅡㅂㄴㅣㄷㅏ',\n",
    "    \n",
    "]\n",
    "lis_wk = [\n",
    "    \n",
    "    ['ㄱㅖ', 'ㅇㅣㅆㅇㅡ'], ['ㅈㅜㅁㅜ','ㅈㅏ'], ['ㅈㅏㅂㅅㅜ','ㅁㅓㄱㅇㅡ']\n",
    "    \n",
    "]\n",
    "lis_end_2low = [\n",
    "    \n",
    "    'ㄷㅓㄹㅏ','ㄴㄷㅏ', 'ㅆㄷㅏ', 'ㄹㅗㄷㅏ', 'ㄷㅏ', 'ㄱㅔ', \n",
    "    'ㄹㅏ',\n",
    "    'ㅇㅑ', \n",
    "    'ㄴㅣㄲㅏ', \n",
    "    'ㄲㅏ', \n",
    "    'ㄴㅣ', \n",
    "    'ㅇㅏ', \n",
    "    \n",
    "]\n",
    "\n",
    "P_LIST = ['.', '?', '!', '\\'', '\\\"']\n",
    "\n",
    "SV_LIST = ['\\'', '\\\"', ':', ';']\n",
    "\n",
    "lis_plus = [\n",
    "    \n",
    "    'EP', 'VCP', \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee13035",
   "metadata": {},
   "outputs": [],
   "source": [
    "mec = Mecab()\n",
    "khai = KhaiiiApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6655d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_h(input, lis_end_h, lis_end_l):\n",
    "    for i in lis_end_h:\n",
    "        if len(input)>=len(i):\n",
    "            if input[-len(i):]==i:\n",
    "                return 1\n",
    "            \n",
    "    for i in lis_end_l:\n",
    "        if len(input)>=len(i):\n",
    "            if input[-len(i):]==i:\n",
    "                return 0\n",
    "            \n",
    "    return -1\n",
    "\n",
    "def unite(input, dict):\n",
    "    for i in dict:\n",
    "        input = re.sub(i[0],i[1],input)\n",
    "    return input\n",
    "    \n",
    "## 자모 단위로 문장을 나누고 합칠 때 쓰는 class ##\n",
    "class Jamodealer:\n",
    "    jamo = []\n",
    "    pp = ''\n",
    "    #각 단어들을 받아와서 자모단위로 나눈다.\n",
    "    def __init__(self,lis_word):\n",
    "    \n",
    "        self.jamo = []\n",
    "        for i in lis_word:\n",
    "            self.jamo.append(split_syllables(i))\n",
    "    \n",
    "    ##사전에서 변환된 자모단위로 분리된 문장을 합칠 때 쓰는 함수이다.     \n",
    "    def make_one(self):\n",
    "        #list 형태로 저장된 자모들의 집합을 하나의 string pp에 저장한다. \n",
    "        self.pp = ''\n",
    "        for i in self.jamo:\n",
    "             self.pp= self.pp+i\n",
    "        ##종성과 종성을 합쳐야 하는 경우가 있다면 합친다.        \n",
    "        self.pp = unite(self.pp, con_dict)\n",
    "        \n",
    "        #자모 단위의 string에서 자모 단위로 사전을 만들고 거기에 index를 부여한다.        \n",
    "        chars = list(set(self.pp))\n",
    "        char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "        ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "        #자모 단위로 분리되었던 문장을 다시 하나로 합친다.\n",
    "        jamo_numbers = [char_to_ix[x] for x in self.pp]\n",
    "        restored_jamo = ''.join([ix_to_char[x] for x in jamo_numbers])\n",
    "        #합쳐진 문장을 return 한다.\n",
    "        restored_text = join_jamos(restored_jamo)\n",
    "        return restored_text\n",
    "\n",
    "def to2lists(input):\n",
    "    lis_word = []\n",
    "    lis_tag = []\n",
    "    #data = han.pos(input,ntags=22,flatten=True, join=False)\n",
    "    data = mec.pos(input)\n",
    "    for i in data:\n",
    "        lis_word.append(i[0])\n",
    "        lis_tag.append(i[1])\n",
    "    return lis_word, lis_tag\n",
    "\n",
    "def to2lists_khaiii(input):\n",
    "    lis_word = []\n",
    "    lis_tag = []\n",
    "    analyzed = khai.analyze(input)  \n",
    "    for data in analyzed:\n",
    "        for morph in data.morphs:\n",
    "            lis_word.append(morph.lex)\n",
    "            lis_tag.append(morph.tag)\n",
    "    return lis_word, lis_tag\n",
    "\n",
    "\n",
    "def rememberSpace(lis, input):\n",
    "    \n",
    "    rlis = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        if lis[i]==input:\n",
    "            rlis.append(i)\n",
    "            \n",
    "    for i in range(len(rlis)):\n",
    "        rlis[i] = rlis[i]-i      \n",
    "    return rlis\n",
    "\n",
    "def convertSpace(lis_space,lis_lis):\n",
    "    \n",
    "    rlis = []\n",
    "    k=0\n",
    "    for i in range(len(lis_lis)):\n",
    "        \n",
    "        if k in lis_space:\n",
    "            rlis.append(i)\n",
    "            \n",
    "        k = k+len(lis_lis[i])\n",
    "        \n",
    "    #print(rlis)  \n",
    "    return rlis\n",
    "\n",
    "def union(lis, lis_lis):\n",
    "    \n",
    "    k = 0\n",
    "    for i in lis:\n",
    "        lis_lis.insert(i+k,' ')\n",
    "        k = k+1\n",
    "\n",
    "def union_t_03(lis_tag):\n",
    "    \n",
    "    for i in range(1, len(lis_tag)):\n",
    "        if lis_tag[i-1] ==' ' or lis_tag[i]==' ':\n",
    "            lis_tag[i] = lis_tag[i]\n",
    "        else:\n",
    "            lis_tag[i] = '/'+lis_tag[i]\n",
    "            \n",
    "def union_w_03(lis_w, lis_tag):\n",
    "    \n",
    "    for i in range(1, len(lis_w)):\n",
    "        if lis_tag[i]==' SF':\n",
    "            lis_w[i] = ' '+lis_w[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c2497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(input):\n",
    "    lis_w, lis_t = to2lists(input)\n",
    "\n",
    "    space_list = rememberSpace(input,' ')\n",
    "    space_location = convertSpace(space_list, lis_w)\n",
    "    union(space_location, lis_w)\n",
    "    union(space_location, lis_t)\n",
    "    union_t_03(lis_t)\n",
    "    union_w_03(lis_w, lis_t)\n",
    "    \n",
    "    str_w = ''\n",
    "    str_t = ''\n",
    "    for i in range(len(lis_w)):\n",
    "        str_w = str_w + lis_w[i]\n",
    "        str_t = str_t + lis_t[i]\n",
    "    \n",
    "    data_w = str_w.split(' ')\n",
    "    data_t = str_t.split(' ')\n",
    "    \n",
    "    lis_word, lis_tag = to2lists(input)\n",
    "    \n",
    "    lis_ind = []\n",
    "    t_ind = 0\n",
    "    jam1 = Jamodealer(lis_word)\n",
    "    jam2 = Jamodealer(data_w)\n",
    "    for i in range(len(data_w)):\n",
    "        element = []\n",
    "        leng = len(data_t[i].split('/'))\n",
    "        res = jam2.jamo[i]\n",
    "        ind = 0\n",
    "        lenlen = 0\n",
    "        #element.append(0)\n",
    "        for j in range(leng):\n",
    "            element.append(ind)\n",
    "            ind = ind + len(jam1.jamo[t_ind])\n",
    "            res = res[len(jam1.jamo[t_ind]):]\n",
    "            \n",
    "            lenlen = len(jam1.jamo[t_ind])+lenlen\n",
    "            t_ind = t_ind+1\n",
    "\n",
    "        element.append(len(jam2.jamo[i]))\n",
    "        lis_ind.append(element)\n",
    "        \n",
    "    return data_w, data_t, lis_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8687bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_khaiii(input):\n",
    "    lis_w, lis_t = to2lists_khaiii(input)\n",
    "\n",
    "    space_list = rememberSpace(input,' ')\n",
    "    space_location = convertSpace(space_list, lis_w)\n",
    "    union(space_location, lis_w)\n",
    "    union(space_location, lis_t)\n",
    "    union_t_03(lis_t)\n",
    "    union_w_03(lis_w, lis_t)\n",
    "    \n",
    "    str_w = ''\n",
    "    str_t = ''\n",
    "    for i in range(len(lis_w)):\n",
    "        str_w = str_w + lis_w[i]\n",
    "        str_t = str_t + lis_t[i]\n",
    "    \n",
    "    data_w = str_w.split(' ')\n",
    "    data_t = str_t.split(' ')\n",
    "    \n",
    "    lis_word, lis_tag = to2lists_khaiii(input)\n",
    "    \n",
    "    lis_ind = []\n",
    "    t_ind = 0\n",
    "    jam1 = Jamodealer(lis_word)\n",
    "    jam2 = Jamodealer(data_w)\n",
    "    for i in range(len(data_w)):\n",
    "        element = []\n",
    "        leng = len(data_t[i].split('/'))\n",
    "        res = jam2.jamo[i]\n",
    "        ind = 0\n",
    "        lenlen = 0\n",
    "        #element.append(0)\n",
    "        for j in range(leng):\n",
    "            element.append(ind)\n",
    "            ind = ind + len(jam1.jamo[t_ind])\n",
    "            res = res[len(jam1.jamo[t_ind]):]\n",
    "            \n",
    "            lenlen = len(jam1.jamo[t_ind])+lenlen\n",
    "            t_ind = t_ind+1\n",
    "\n",
    "        element.append(len(jam2.jamo[i]))\n",
    "        lis_ind.append(element)\n",
    "        \n",
    "    return data_w, data_t, lis_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0f8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_beta_khaiii(input, lis_ef, tag_last, lis_w_last, lis_w_last_not):\n",
    "    data_w, data_t, lis_ind = prepro_khaiii(input)\n",
    "    \n",
    "    last_words = []\n",
    "\n",
    "    data_w_jamo = []\n",
    "\n",
    "    data_t_after = []\n",
    "    \n",
    "    lis_target_ind = []\n",
    "    \n",
    "    for i in data_w:\n",
    "        jam_ele = Jamodealer(i)\n",
    "        ele = ''\n",
    "        for j in jam_ele.jamo:\n",
    "            ele = ele+j\n",
    "        data_w_jamo.append(ele)\n",
    "    \n",
    "    for i in range(len(data_t)):\n",
    "        #if i<len(data_t)-1:\n",
    "        if i<len(data_t):\n",
    "            lis_res = []\n",
    "            for ind in range(len(lis_ind[i])-1):\n",
    "                lis_res.append(data_w_jamo[i][lis_ind[i][ind]:lis_ind[i][ind+1]])\n",
    "\n",
    "        \n",
    "        if 'EF/SF' in data_t[i] or 'EF/SV' in data_t[i] or 'UNKNOWN/SF' in data_t[i] or 'UNKNOWN/SV' in data_t[i]:# and 'EC/SF' not in data_t[i]:\n",
    "            if 'EF/SF' in data_t[i] or 'UNKNOWN' in data_t[i]:\n",
    "                elements = data_t[i].split('/')\n",
    "                flag = 0\n",
    "\n",
    "                for j in range(len(elements)):\n",
    "                \n",
    "                    flag_end = detect_h(lis_res[j], lis_w_last,  lis_w_last_not)\n",
    "\n",
    "                    if elements[j] in lis_ef and flag_end==1: #and j == len(elements)-1:\n",
    "\n",
    "                        elements[j] = 'NULL'\n",
    "                    \n",
    "                        last_words.append(data_w_jamo[i][lis_ind[i][j]:lis_ind[i][j+1]])\n",
    "                        lis_res[j]=''\n",
    "                    \n",
    "                        lis_target_ind.append(i)\n",
    "                    \n",
    "#                         elements_post = '/'.join(elements)\n",
    "#                         data_t_after.append(elements_post)\n",
    "                        \n",
    "                    elif 'EF' in elements[j] and flag_end==1:# + EF를 처리하는 부분이므로 + EF 만을 마지막에서 처리한다.\n",
    "                        for jam in lis_w_last:\n",
    "                            if len(lis_res[j])>=len(jam):\n",
    "\n",
    "                                res_out_punc = lis_res[:lis_ind[i][-2]][j]\n",
    "\n",
    "                                if res_out_punc[-len(jam):]==jam:\n",
    "\n",
    "                                    #print(jam)\n",
    "                                    lis_target_ind.append(i)\n",
    "                                \n",
    "                                    last_words.append(jam)\n",
    "                                    lis_res[j] = lis_res[j].replace(jam, '', 1)\n",
    "\n",
    "                                    for k in tag_last:\n",
    "\n",
    "                                        if k in elements[j]:\n",
    "                                        \n",
    "                                            if '+' in elements[j]:\n",
    "                                            \n",
    "                                                ind = elements[j].index('+'+k)\n",
    "                                                elements[j] = elements[j][:ind]\n",
    "                                    break\n",
    "                                \n",
    "                                elif lis_w_last.index(jam)==len(lis_w_last)-1:#new\n",
    "                                    for k in tag_last:\n",
    "\n",
    "                                        if k in elements[j]:\n",
    "                                        \n",
    "                                            if '+' in elements[j]:\n",
    "                                            \n",
    "                                                ind = elements[j].index('+'+k)\n",
    "                                                elements[j] = elements[j][:ind]\n",
    "                                            \n",
    "                                    lis_target_ind.append(i)\n",
    "                                    last_words.append('')\n",
    "                                    break\n",
    "\n",
    "                \n",
    "                elements_post = '/'.join(elements)\n",
    "                data_t_after.append(elements_post)\n",
    "                \n",
    "                data_w_jamo[i] = ''.join(lis_res)\n",
    "\n",
    "                #elements_post = '/'.join(elements)\n",
    "                #data_t_after.append(elements_post)\n",
    "                    \n",
    "            #######################################\n",
    "            \n",
    "            #data_t_after.append(data_t[i])\n",
    "            \n",
    "        elif 'EC/SF' in data_t[i] or 'JX/SF' in data_t[i]:\n",
    "            #print('ee')\n",
    "            elements = data_t[i].split('/')\n",
    "            \n",
    "            flag = 0\n",
    "            for j in range(len(elements)):\n",
    "                \n",
    "                flag_end = -1\n",
    "                if 'EC' in elements[j] or 'JX' in elements[j]:\n",
    "                    flag_end = detect_h(lis_res[j], lis_w_last, lis_w_last_not)\n",
    "\n",
    "                #print(flag_end)\n",
    "                if flag_end==1 and i not in lis_target_ind:\n",
    "                    for jam in lis_w_last:\n",
    "\n",
    "                        if len(jam)<=len(lis_res[j]):\n",
    "                            #print(lis_res[j])\n",
    "                            if lis_res[j][-len(jam):]==jam:\n",
    "\n",
    "                                last_words.append(jam)\n",
    "                                lis_res[j] = lis_res[j].replace(jam, '', 1)\n",
    "                                #print(lis_res[j])\n",
    "                    lis_target_ind.append(i)\n",
    "                    #last_words.append('')\n",
    "            data_t_after.append(data_t[i])\n",
    "            data_w_jamo[i] = ''.join(lis_res)\n",
    "        ####### #######\n",
    "            \n",
    "        else:\n",
    "            data_t_after.append(data_t[i])\n",
    "            \n",
    "    \n",
    "    \n",
    "        lis_normal = []\n",
    "    \n",
    "        for i in data_w_jamo:\n",
    "            jam_n = Jamodealer(i)\n",
    "            lis_normal.append(jam_n.make_one())\n",
    "    \n",
    "        \n",
    "        \n",
    "    for i in range(len(lis_target_ind)):\n",
    "        if 'ㅅㅔㅇㅛ' == last_words[i] or 'ㄹㄹㅐㅇㅛ' == last_words[i]:\n",
    "            \n",
    "            for wk in lis_wk:\n",
    "                #print(data_w_jamo[lis_target_ind[i]][-len(wk[0])-1:-1])\n",
    "                if data_w_jamo[lis_target_ind[i]][-len(wk[0])-1:-1] ==wk[0]:\n",
    "                    #print('rrrr')\n",
    "                    ele = data_w_jamo[lis_target_ind[i]][:-len(wk[0])-1]\n",
    "                    ele = ele + wk[1]\n",
    "                    ele = ele + data_w_jamo[lis_target_ind[i]][-1]\n",
    "                    data_w_jamo[lis_target_ind[i]] = ele\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "    return data_w, data_t, lis_ind, data_w_jamo, data_t_after, last_words, lis_target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05dc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_beta_09(input, lis_ef, tag_last, lis_w_last, lis_w_last_not):\n",
    "    data_w, data_t, lis_ind = prepro(input)\n",
    "    \n",
    "    last_words = []\n",
    "\n",
    "    data_w_jamo = []\n",
    "\n",
    "    data_t_after = []\n",
    "    \n",
    "    lis_target_ind = []\n",
    "    \n",
    "    for i in data_w:\n",
    "        jam_ele = Jamodealer(i)\n",
    "        ele = ''\n",
    "        for j in jam_ele.jamo:\n",
    "            ele = ele+j\n",
    "        data_w_jamo.append(ele)\n",
    "    \n",
    "    for i in range(len(data_t)):\n",
    "        #if i<len(data_t)-1:\n",
    "        if i<len(data_t):\n",
    "            lis_res = []\n",
    "            for ind in range(len(lis_ind[i])-1):\n",
    "                lis_res.append(data_w_jamo[i][lis_ind[i][ind]:lis_ind[i][ind+1]])\n",
    "\n",
    "        \n",
    "        if 'EF/SF' in data_t[i] or 'EF/SV' in data_t[i] or 'UNKNOWN/SF' in data_t[i] or 'UNKNOWN/SV' in data_t[i]:# and 'EC/SF' not in data_t[i]:\n",
    "            if 'EF/SF' in data_t[i] or 'UNKNOWN' in data_t[i]:\n",
    "                elements = data_t[i].split('/')\n",
    "                flag = 0\n",
    "\n",
    "                for j in range(len(elements)):\n",
    "                \n",
    "                    flag_end = detect_h(lis_res[j], lis_w_last,  lis_w_last_not)\n",
    "\n",
    "                    if elements[j] in lis_ef and flag_end==1: #and j == len(elements)-1:\n",
    "\n",
    "                        elements[j] = 'NULL'\n",
    "                    \n",
    "                        last_words.append(data_w_jamo[i][lis_ind[i][j]:lis_ind[i][j+1]])\n",
    "                        lis_res[j]=''\n",
    "                    \n",
    "                        lis_target_ind.append(i)\n",
    "                    \n",
    "#                         elements_post = '/'.join(elements)\n",
    "#                         data_t_after.append(elements_post)\n",
    "                        \n",
    "                    elif 'EF' in elements[j] and flag_end==1:# + EF를 처리하는 부분이므로 + EF 만을 마지막에서 처리한다.\n",
    "                        for jam in lis_w_last:\n",
    "                            if len(lis_res[j])>=len(jam):\n",
    "\n",
    "                                res_out_punc = lis_res[:lis_ind[i][-2]][j]\n",
    "\n",
    "                                if res_out_punc[-len(jam):]==jam:\n",
    "\n",
    "                                    lis_target_ind.append(i)\n",
    "                                \n",
    "                                    last_words.append(jam)\n",
    "                                    lis_res[j] = lis_res[j].replace(jam, '', 1)\n",
    "\n",
    "                                    for k in tag_last:\n",
    "\n",
    "                                        if k in elements[j]:\n",
    "                                        \n",
    "                                            if '+' in elements[j]:\n",
    "                                            \n",
    "                                                ind = elements[j].index('+'+k)\n",
    "                                                elements[j] = elements[j][:ind]\n",
    "                                    break\n",
    "                                \n",
    "                                elif lis_w_last.index(jam)==len(lis_w_last)-1:#new\n",
    "                                    for k in tag_last:\n",
    "\n",
    "                                        if k in elements[j]:\n",
    "                                        \n",
    "                                            if '+' in elements[j]:\n",
    "                                            \n",
    "                                                ind = elements[j].index('+'+k)\n",
    "                                                elements[j] = elements[j][:ind]\n",
    "                                            \n",
    "                                    lis_target_ind.append(i)\n",
    "                                    last_words.append('')\n",
    "                                    break\n",
    "\n",
    "                \n",
    "                elements_post = '/'.join(elements)\n",
    "                data_t_after.append(elements_post)\n",
    "                \n",
    "                data_w_jamo[i] = ''.join(lis_res)\n",
    "\n",
    "                #elements_post = '/'.join(elements)\n",
    "                #data_t_after.append(elements_post)\n",
    "                    \n",
    "            #######################################\n",
    "            \n",
    "            #data_t_after.append(data_t[i])\n",
    "            \n",
    "        elif 'EC/SF' in data_t[i] or 'JX/SF' in data_t[i]:\n",
    "            elements = data_t[i].split('/')\n",
    "            \n",
    "            flag = 0\n",
    "            for j in range(len(elements)):\n",
    "                \n",
    "                flag_end = -1\n",
    "                if 'EC' in elements[j] or 'JX' in elements[j]:\n",
    "                    flag_end = detect_h(lis_res[j], lis_w_last, lis_w_last_not)\n",
    "\n",
    "                if flag_end==1 and i not in lis_target_ind:\n",
    "                    for jam in lis_w_last:\n",
    "\n",
    "                        if len(jam)<=len(lis_res[j]):\n",
    "                            if lis_res[j][-len(jam):]==jam:\n",
    "\n",
    "                                last_words.append(jam)\n",
    "                                lis_res[j] = lis_res[j].replace(jam, '', 1)\n",
    "                                \n",
    "                    lis_target_ind.append(i)\n",
    "                    #last_words.append('')\n",
    "            data_t_after.append(data_t[i])\n",
    "            data_w_jamo[i] = ''.join(lis_res)\n",
    "        ####### #######\n",
    "            \n",
    "        else:\n",
    "            data_t_after.append(data_t[i])\n",
    "            \n",
    "    \n",
    "        if 'EP/EF' in data_t[i]:\n",
    "            elements = data_t[i].split('/')\n",
    "            \n",
    "            flag = 0\n",
    "            for j in range(len(elements)):\n",
    "                \n",
    "                flag_end = -1\n",
    "                if elements[j]=='EP':\n",
    "                    if lis_res[j]=='ㅅㅣ':\n",
    "                        flag_end =0\n",
    "                        \n",
    "                if flag_end==0 and i in lis_target_ind:\n",
    "                    \n",
    "                    last_words[-1] = 'ㅅㅣ'+last_words[-1]\n",
    "                    lis_res[j]=''\n",
    "            data_w_jamo[i] = ''.join(lis_res)\n",
    "        lis_normal = []\n",
    "    \n",
    "        for i in data_w_jamo:\n",
    "            jam_n = Jamodealer(i)\n",
    "            lis_normal.append(jam_n.make_one())\n",
    "    \n",
    "        \n",
    "        \n",
    "    for i in range(len(lis_target_ind)):\n",
    "        if 'ㅅㅔㅇㅛ' == last_words[i] or 'ㄹㄹㅐㅇㅛ' == last_words[i]:\n",
    "            \n",
    "            for wk in lis_wk:\n",
    "                #print(data_w_jamo[lis_target_ind[i]][-len(wk[0])-1:-1])\n",
    "                if data_w_jamo[lis_target_ind[i]][-len(wk[0])-1:-1] ==wk[0]:\n",
    "                    \n",
    "                    ele = data_w_jamo[lis_target_ind[i]][:-len(wk[0])-1]\n",
    "                    ele = ele + wk[1]\n",
    "                    ele = ele + data_w_jamo[lis_target_ind[i]][-1]\n",
    "                    data_w_jamo[lis_target_ind[i]] = ele\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "    return data_w, data_t, lis_ind, data_w_jamo, data_t_after, last_words, lis_target_ind\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc9696fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "######높임말 -> 반말\n",
    "#현재 만들어진 것은 EF만 잘라낼 것이다. \n",
    "#원래 ef사전에 mapping 되는 것을 찾아낸다.\n",
    "# EF_1 = {\n",
    "#     ['ㅂ','ㄴ','ㅣ','ㄷ','ㅏ']:['ㄷ','ㅏ'],\n",
    "#     ['ㅅ','ㅡ','ㅂ','ㄴ','ㅣ','ㄷ','ㅏ']:['ㄷ','ㅏ']\n",
    "# }\n",
    "#python dictionary로 접근\n",
    "#종결어미 처리\n",
    "EF = {\n",
    "    ###하십시오체###\n",
    "    #평서문\n",
    "    #'ㅂㄴㅣㄷㅏ': 'ㄷㅏ',\n",
    "    'ㅂㄴㅣㄷㅏ': 'special3',\n",
    "    'ㅅㅡㅂㄴㅣㄷㅏ':'special2',\n",
    "    'ㅇㅗㄹㅅㅣㄷㅏ':'ㄷㅏ', #**\n",
    "    'ㅂㅈㅣㅇㅛ':'지', #**\n",
    "    'ㅅㅣㅂㄴㅣㄷㅏ':'special1',\n",
    "    'ㅇㅡㅅㅣㅂㄴㅣㄷㅏ':'ㅇㅡㅅㅣㄴㄷㅏ',\n",
    "    'ㅇㅡㅅㅣㅂㄴㅣㄲㅏ':'ㅇㅡㅅㅣㄴㅣ',\n",
    "\n",
    "    #의문문\n",
    "    'ㅅㅡㅂㄴㅣㄲㅏ':'ㄴㅣ',\n",
    "    'ㅂㄴㅣㄲㅏ': 'ㄴㅣ',\n",
    "    'ㅅㅣㅂㄴㅣㄲㅏ':'special1', #EP+EF\n",
    "    #명령법\n",
    "    'ㅇㅡㅅㅔㅇㅛ': 'special0',\n",
    "    'ㅅㅔㅇㅛ':'special1',\n",
    "    'ㅅㅣㅇㅓㅇㅛ': 'special1',\n",
    "    'ㅅㅣㅂㅅㅣㅇㅗ':'ㅅㅣㅇㅗ',\n",
    "    #청유법\n",
    "    'ㅂㅅㅣㄷㅏ':'special4',\n",
    "    'ㅇㅡㅂㅅㅣㄷㅏ':'special4',\n",
    "    ###하오체###\n",
    "    \n",
    "    ###해요체###\n",
    "    #평서문\n",
    "    'ㅇㅓㅇㅛ':'ㅇㅓ',\n",
    "    'ㅇㅏㅇㅛ':'ㅇㅏ',\n",
    "    'ㅈㅛ':'ㅈㅣ',\n",
    "    'ㅇㅔㅇㅛ':'ㅇㅑ',\n",
    "    'ㅇㅖㅇㅛ':'ㅇㅑ',\n",
    "    'ㅇㅛ':'special5',\n",
    "    'ㄷㅐㅇㅛ':'ㄷㅐ',\n",
    "    'ㄷㅔㅇㅛ':'ㄷㅔ',\n",
    "    'ㄴㅔㅇㅛ':'ㄴㅔ',\n",
    "    'ㄴㅡㄴㄷㅔㅇㅛ':'ㄴㅡㄴㄷㅔ',\n",
    "    'ㄱㅓㄷㅡㄴㅇㅛ':'ㄱㅓㄷㅡㄴ',\n",
    "    'ㄱㅜㄴㅇㅛ': 'ㄱㅜㄴㅏ',\n",
    "    'ㅇㅡㄴㄷㅔㅇㅛ':'ㅇㅡㄴㄷㅔ',\n",
    "    'ㅈㅏㄱㅜㅇㅛ':'ㅈㅏㄱㅜ',\n",
    "    'ㄴㅣㄲㅏㅇㅛ': 'ㄴㅣㄲㅏ',\n",
    "    'ㅈㅣㅇㅛ':'ㅈㅣ',\n",
    "    #의문문\n",
    "    'ㄴㅏㅇㅛ':'ㄴㅣ',\n",
    "    'ㄹㄲㅏㅇㅛ':'ㄹㄲㅏ',\n",
    "    'ㅇㅡㄹㄲㅏㅇㅛ':'ㅇㅡㄹㄲㅏ',\n",
    "}\n",
    "need_origin_EF = {\n",
    "    'ㅂㅅㅣㄷㅏ':'ㅈㅏ',\n",
    "}\n",
    "#'ㄹ'규칙 활용 -> ㄹ 규칙 활용이 일어나는 동사들을 최대한 모아두고, 만약 하나의 단어에 여러 의미가 담긴다면?\n",
    "EF_R_rule= {\n",
    "    'ㄱㅜ':'ㄹ',\n",
    "    'ㄴㅗ':'ㄹ',\n",
    "    'ㄴㅏ':'ㄹ',\n",
    "    'ㄷㅗ':'ㄹ',\n",
    "    'ㄷㅡ':'ㄹ',\n",
    "    'ㄷㅏ':'ㄹ',\n",
    "    'ㄷㅜ':'ㄹ',\n",
    "    'ㅂㅜ':'ㄹ',\n",
    "    'ㄲㅗ':'ㄹ',\n",
    "    'ㅁㅣ':'ㄹ',\n",
    "    'ㅁㅜ':'ㄹ',\n",
    "    #'ㅂㅗㅍㅜ':'ㄹ', #error predicate 수정\n",
    "    'ㅂㅜ':'ㄹ',\n",
    "    'ㅅㅡ':'ㄹ',\n",
    "    'ㄸㅓ':'ㄹ',   \n",
    "}\n",
    "\n",
    "####EXAMPLE####\n",
    "#tag = 'ㅅㅔㅇㅛ'\n",
    "# sentence = ['ㄱㅡㄴㅡㄴ', 'ㅅㅏㄴㅇㅡㄹ', 'ㅇㅗㄹㅡ']\n",
    "# tagList = ['NP/JX', 'NNG/JKO', 'VV/']\n",
    "\n",
    "# tag2 = 'ㅅㅔㅇㅛ'\n",
    "# sentence2 = ['ㄱㅡㄴㅡㄴ', 'ㅂㅐㄹㄱㅗㅍㅡ']\n",
    "# tagList2 = ['NP/JX', 'VA/']\n",
    "####EXAMPLE####\n",
    "def treatSF(stc, ex):\n",
    "    ind_point = -1\n",
    "    point = ''          \n",
    "    for i in range(len(stc)):\n",
    "        if stc[i] in P_LIST:\n",
    "            point = stc[i]\n",
    "            break\n",
    "            \n",
    "    if point in P_LIST:\n",
    "        ind_point = stc.index(point)\n",
    "    \n",
    "    r_word = ''\n",
    "    r_pun = ''\n",
    "    \n",
    "    if ind_point!=-1:\n",
    "        r_word = stc[:ind_point]\n",
    "        r_pun = stc[ind_point:]\n",
    "    else:\n",
    "        r_word = stc\n",
    "    return r_word+ex+r_pun\n",
    "\n",
    "def delete_EP_si(stn, taglist):\n",
    "    si = stn[-3:-1]\n",
    "    eusi = stn[-5:-1]\n",
    "    check_si = taglist[-11:-2]\n",
    "    #print(si, eusi, check_si)\n",
    "    result =''\n",
    "    flag = 0\n",
    "    if taglist.find('SF') !=-1:\n",
    "        if (eusi =='ㅇㅡㅅㅣ') and (check_si.find('EP+EP')!=-1 or check_si.find('EP/NULL')!=-1):\n",
    "            result = stn[:-5]+stn[-1]\n",
    "            flag = 1\n",
    "        elif (si =='ㅅㅣ') and (check_si.find('/EP/')!=-1):\n",
    "            result = stn[:-3]+stn[-1]\n",
    "            #print('s')\n",
    "\n",
    "    return result, flag\n",
    "\n",
    "def check_VV_VA(sentence, tag):\n",
    "    #t= tag[-6:-3]\n",
    "    #t = tag[:2]\n",
    "    t = tag\n",
    "    print(t, tag)\n",
    "    if 'VV' in t or 'VX' in t or 'XSV' in t or ('VV/EP' in tag and 'ㅅㅣ' in sentence[-5:-1]) :\n",
    "        return 1\n",
    "    elif 'VA' in t:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "## 'ㅏ', 'ㅗ' 처리\n",
    "def convertSpecialCase_AhOh(sentence):\n",
    "    #print(sentence[-1][-2:])\n",
    "    #print(sentence[-1][-4:])\n",
    "    ## 수정할 필요 있음!\n",
    "    if sentence[-3:-1] == 'ㅍㅡ' or sentence[-3:-1] == 'ㅃㅡ':\n",
    "        if sentence[-5:-3].find('ㅏ') !=-1 or sentence[-5:-3].find('ㅗ')!=-1:\n",
    "            return 'ㅏ'\n",
    "        else:\n",
    "            return 'ㅓ'\n",
    "    elif sentence[-3:].find('ㅏ') !=-1 or sentence[-3:].find('ㅗ') !=-1:\n",
    "        return 'ㅇㅏ'\n",
    "    else:\n",
    "        return 'ㅇㅓ'\n",
    "\n",
    "#'ㅇㅗㄹㅡ'\n",
    "def convertSpecialCase_SaeYo(sentence, ending, tag):\n",
    "    result = ''\n",
    "    end_EF=''\n",
    "    final=''\n",
    "    pun = sentence[-1:]\n",
    "    predicate = sentence[-3:-1]\n",
    "    stem = sentence[:-3]\n",
    "    isVcp = tag[-11:]\n",
    "    #print('문장아 나와라',pun, 'ssdads')\n",
    "    #만약 VCP가 있다면 '야'를 붙이고 return 한다.\n",
    "    if isVcp.find('VCP') != -1:\n",
    "        final = 'ㅇㅑ'\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "    # 'ㄹ'규칙 활용\n",
    "    elif predicate in EF_R_rule:\n",
    "        result= EF_R_rule[predicate]\n",
    "        #sentence[-1] += result\n",
    "        ##'아' 또는 '어' 로 처리\n",
    "        end_EF = convertSpecialCase_AhOh(sentence)\n",
    "        #sentence.append(end_EF)\n",
    "        final = result +end_EF\n",
    "        #print(final)\n",
    "    #'르' 불규칙 활용\n",
    "    elif predicate =='ㄹㅡ':\n",
    "        # 용언 종성에 ㄹ이 있다면\n",
    "        #sentence[-1] = sentence[-1].replace('ㄹㅡ','')\n",
    "        predicate = predicate.replace('ㄹㅡ','')\n",
    "        sentence = stem+predicate + pun\n",
    "        end_EF = convertSpecialCase_AhOh(sentence)\n",
    "        end_EF = end_EF[-1]\n",
    "        #print(end_EF, sentence, 'sss')\n",
    "        if sentence[-1].find('ㄹ') != -1:\n",
    "            final = 'ㄹ'+end_EF\n",
    "        else:\n",
    "            final  = 'ㄹㄹ'+end_EF\n",
    "        \n",
    "    #'우' 불규칙 활용\n",
    "    # '푸'를 제외한 다른 'ㅜ'는 'ㅓ'와 결합\n",
    "    elif predicate.find('ㅍㅜ') !=-1:\n",
    "        #sentence[-1] = sentence[-1].replace('ㅜ','ㅓ')\n",
    "        predicate = predicate.replace('ㅜ','')\n",
    "        sentence = stem+predicate + pun\n",
    "        final = 'ㅓ'\n",
    "        #final = sentence[-1][-1:]\n",
    "    #'오' 불규칙 활용(고려하지 않을 수 있음)\n",
    "    #'하' 불규칙 활용\n",
    "    elif predicate.find('ㅎㅏ') !=-1:\n",
    "        #sentence[-1] = sentence[-1].replace('ㅏ','ㅐ')\n",
    "        predicate = predicate.replace('ㅏ','')\n",
    "        sentence = stem + predicate + pun\n",
    "        final = 'ㅐ'\n",
    "    #활용이 안되었던 용언 처리\n",
    "    else:\n",
    "        if predicate.find('ㅡ') !=-1:\n",
    "            end_EF = convertSpecialCase_AhOh(sentence)\n",
    "            #sentence[-1] = sentence[-1].replace('ㅡ',end_EF)\n",
    "            predicate = predicate.replace('ㅡ','')\n",
    "            sentence = stem + predicate + pun\n",
    "            #print(sentence, '처리후')\n",
    "            final = end_EF[-1]\n",
    "        ## 수정할 필요 있음!!\n",
    "        elif predicate.find('ㅗ') !=-1:\n",
    "            #sentence[-1] = sentence[-1].replace('ㅗ','ㅘ')\n",
    "            predicate = predicate.replace('ㅗ','')\n",
    "            sentence = stem+predicate+ pun\n",
    "            final = 'ㅘ'\n",
    "        elif predicate.find('ㅜ') !=-1:\n",
    "            #sentence[-1] = sentence[-1].replace('ㅜ','ㅝ')\n",
    "            predicate = predicate.replace('ㅜ','') \n",
    "            sentence = stem + predicate+ pun\n",
    "            final = 'ㅝ'\n",
    "        elif predicate.find('ㅣ') !=-1:\n",
    "            #sentence[-1] = sentence[-1].replace('ㅣ','ㅕ')\n",
    "#             if predicate == 'ㅇㅣ':\n",
    "#                 sentence = sentence.replace('ㅇㅣ','')\n",
    "            predicate = predicate.replace('ㅣ','')\n",
    "            sentence = stem+predicate+ pun\n",
    "            final = 'ㅕ'\n",
    "        else:\n",
    "            final = convertSpecialCase_AhOh(sentence)\n",
    "            #sentence = sentence.append(final)\n",
    "            if  predicate.find('ㅏ') !=-1:\n",
    "                final = ''\n",
    "            elif  predicate.find('ㅓ') !=-1:\n",
    "                final = ''\n",
    "            elif  predicate.find('ㅐ') !=-1:\n",
    "                final = ''\n",
    "#                 return final, sentence\n",
    "#             return final, sentence\n",
    "#     return final, sentence\n",
    "    converted = treatSF(sentence, final)\n",
    "    #print(converted)\n",
    "    return converted\n",
    "    \n",
    "def convertSpecialCase_Da(sentence, ending, taglist):\n",
    "    #print(sentence, ending, taglist)\n",
    "    final =''\n",
    "    if (sentence.find('ㅇㅣㅆ') !=-1 or sentence.find('ㅇㅏㄶ') !=-1)and (taglist.find('VV/NULL') !=-1 or taglist.find('VX/NULL') !=-1):\n",
    "        final = 'ㄷㅏ'\n",
    "    elif taglist.find('EP/NULL') !=-1 or taglist.find('VA/NULL') !=-1:\n",
    "        #print('다로 변경할 것')\n",
    "        final = 'ㄷㅏ'\n",
    "    elif taglist.find('VV/NULL') !=-1:\n",
    "        final = 'ㄴㅡㄴㄷㅏ'\n",
    "    elif taglist.find('VV') !=-1:\n",
    "        #print('현재형 동사가 왔으므로, 는다로 변경할 것')\n",
    "        final = 'ㄴㅡㄴㄷㅏ'\n",
    "    converted = treatSF(sentence, final)\n",
    "    return converted\n",
    "\n",
    "#def convertSpecialCase_Nida(sentence, ending, taglist, kh_stn, kh_end, kh_tag):\n",
    "def convertSpecialCase_Nida(sentence, ending, taglist):\n",
    "    #print(sentence, ending, taglist)\n",
    "    #시가 있으면 ㄴ다를 붙인다. \n",
    "    #형용사, 서술격 조사일 경우 convertSpecialCase_SaeYo를 통해 변경한 다음, 아/어를 제거 후 다를 붙이고\n",
    "    #동사일 경우 ㄴ다를 붙여서 해결한다. \n",
    "    predicate = check_VV_VA(sentence, taglist)\n",
    "    #VV\n",
    "    if predicate == 1:\n",
    "        final = 'ㄴㄷㅏ'\n",
    "        converted = treatSF(sentence, final)\n",
    "    #VA\n",
    "    #형용사의 경우, 세요를 거친 후 마지막을 붙인다면 이상해질 수 있다. 그냥 khaiii를 쓰는 것이 안전하다고 판단된다. \n",
    "    elif predicate == 0:\n",
    "        final = 'ㄷㅏ'\n",
    "        #print(treatSF(sentence,ending))\n",
    "        temp=treatSF(sentence,ending)\n",
    "        jam1 = Jamodealer(temp)\n",
    "        s = jam1.make_one()\n",
    "        converted_kh = proc_khaiii(s)\n",
    "        converted_kh = converted_kh.replace('ㅂ니다', '')\n",
    "        #print(converted_kh)\n",
    "        converted = treatSF(converted_kh, final)\n",
    "    else:\n",
    "        final = 'ㄷㅏ'\n",
    "        converted = treatSF(sentence, final)\n",
    "    return converted\n",
    "def convertSpecialCase_Yo(sentence, ending, tag):\n",
    "    pun = sentence[-1:]\n",
    "    predicate = sentence[-3:-1]\n",
    "    stem = sentence[:-3]\n",
    "    isVcp = tag[-11:]\n",
    "    #print(sentence, ending)\n",
    "    temp =''\n",
    "    ni = 0\n",
    "    after_si, ni = delete_EP_si(sentence, tag)\n",
    "    if after_si != '':\n",
    "        sentence = after_si\n",
    "        after_si =''\n",
    "    \n",
    "    if isVcp.find('VCP') != -1:\n",
    "        final = 'ㅇㅑ'\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "    else:\n",
    "        final = ''\n",
    "        converted = treatSF(sentence, final)\n",
    "        return converted\n",
    "    \n",
    "##if Verb & adjective\n",
    "## '-시' 등 선어말 처리\n",
    "## 습니다는 동사면 '는다', 그외에는 '다'로 간다\n",
    "def rememberSpace_k(lis, input):\n",
    "    \n",
    "    rlis = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        if lis[i]==input:\n",
    "            rlis.append(i)\n",
    "            \n",
    "    for i in range(len(rlis)):\n",
    "        rlis[i] = rlis[i]-i      \n",
    "    return rlis\n",
    "\n",
    "def convertSpace_k(lis_space,lis_lis):\n",
    "    \n",
    "    rlis = []\n",
    "    k=0\n",
    "    for i in range(len(lis_lis)):\n",
    "        \n",
    "        if k in lis_space:\n",
    "            rlis.append(i)\n",
    "            \n",
    "        k = k+len(lis_lis[i])\n",
    "        \n",
    "    #print(rlis)  \n",
    "    return rlis\n",
    "\n",
    "def proc_khaiii(input):\n",
    "    \n",
    "    r_sen = input\n",
    "    \n",
    "    res = ''\n",
    "    slis = []\n",
    "    for i in range(len(input)):\n",
    "        if r_sen[i]==' ':\n",
    "            slis.append(1)\n",
    "        elif r_sen[i]=='  ':\n",
    "            slis.append(2)\n",
    "            \n",
    "    wlis = r_sen.split(' ')\n",
    "    \n",
    "    uu = khai.analyze(wlis[0])\n",
    "    \n",
    "    elem = ''\n",
    "    \n",
    "    for data in uu[0].morphs:\n",
    "        elem = elem + data.lex\n",
    "    \n",
    "    res = res+elem\n",
    "    \n",
    "    for i in range(len(slis)):\n",
    "        elem = ''\n",
    "        elem = elem+slis[i]*' '\n",
    "        if i != len(wlis)-1:\n",
    "            uu = khai.analyze(wlis[i+1])\n",
    "            for data in uu[0].morphs:\n",
    "                elem = elem+data.lex\n",
    "        res = res+elem\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bdadacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Changer(object):\n",
    "    \n",
    "    #def make_end_low(self, sentence, ending, taglist, kstn, kend, ktag):\n",
    "    def make_end_low(self, sentence, ending, taglist):\n",
    "        re_value =''\n",
    "        temp =''\n",
    "        ni = 0\n",
    "        flag = 0\n",
    "        #존칭 동사 또는 보조 동사를 파악하고 이를 변환 전에 미리 바꿔주어야 한다. \n",
    "        for key in EF:\n",
    "            if ending == key:\n",
    "                flag = 1\n",
    "                re_value = EF[key]\n",
    "                #나요, 으세요(으세요, 세요 모두 함수에서 커버 가능), 습니까 case\n",
    "                if re_value == 'special0':\n",
    "                    #시처리하기\n",
    "                    after_si, ni = delete_EP_si(sentence, taglist)\n",
    "                    if after_si != '':\n",
    "                        sentence = after_si\n",
    "                        after_si =''\n",
    "                    temp = convertSpecialCase_AhOh(sentence)\n",
    "                    re_value = treatSF(sentence, temp)\n",
    "                    #ㅗ,ㅜ 이면 ㅘ, ㅝ로 결합할 것\n",
    "                    #print(re_value)\n",
    "                #-세요,십니다, 십니까\n",
    "                elif re_value == 'special1':\n",
    "                    #시처리하기\n",
    "                    after_si, ni = delete_EP_si(sentence, taglist)\n",
    "                    if after_si != '':\n",
    "                        sentence = after_si\n",
    "                        after_si =''\n",
    "                    re_value = convertSpecialCase_SaeYo(sentence, ending, taglist)\n",
    "                elif re_value == 'special2':\n",
    "                    re_value = convertSpecialCase_Da(sentence, ending, taglist)\n",
    "                elif re_value == 'special3':\n",
    "                    #시처리 안함 '시' 보존\n",
    "                    #re_value = convertSpecialCase_Nida(sentence, ending, taglist, kstn, kend, ktag)\n",
    "                    re_value = convertSpecialCase_Nida(sentence, ending, taglist)\n",
    "                elif re_value == 'special4':\n",
    "                    re_value = convertSpecialCase_SaeYo(sentence, ending, taglist)\n",
    "                    if ending == 'ㅇㅡㅂㅅㅣㄷㅏ':\n",
    "                        temp = convertSpecialCase_AhOh(sentence)\n",
    "                        re_value = treatSF(sentence, temp)\n",
    "                elif re_value == 'special5':\n",
    "                    re_value = convertSpecialCase_Yo(sentence, ending, taglist)\n",
    "                else:\n",
    "                    #위험하기 때문에 데이터 확인 후 수정\n",
    "#                     if taglist.find('EP') !=-1 and sentence[-4:].find('ㅅㅣ.'):\n",
    "#                         sentence = delete_EP_si(sentence, taglist)\n",
    "#                         re_value = convertSpecialCase_SaeYo(sentence, ending)\n",
    "#                     else:\n",
    "#                         re_value = treatSF(sentence,re_value)\n",
    "                    re_value = treatSF(sentence,re_value)\n",
    "        if flag ==0:\n",
    "            return treatSF(sentence, ending)\n",
    "        # print(sentence)\n",
    "        return re_value\n",
    "    \n",
    "\n",
    "    \n",
    "    def to_low(self, input):\n",
    "        result = input\n",
    "        space_list = rememberSpace(input,' ')\n",
    "        \n",
    "        test_w, test_t = to2lists(result)\n",
    "        pre_w, pre_t, pre_ind = prepro(result)\n",
    "        \n",
    "#         data_w, data_t, lis_i, lis_w, lis_t, off_word, lis_target_ind = prepro_beta_05(result, lis_beta_ef, lis_tag_last, lis_end_2low, lis_end)\n",
    "        #kori_w, kori_t, kind, kconverted_w, kconverted_t, klast_ef,ktarget_ind  = prepro_beta_khaiii(result,lis_beta_ef_h, lis_tag_last, lis_end, lis_end_2low)\n",
    "        ori_w, ori_t, ind, converted_w, converted_t, last_ef,target_ind  = prepro_beta_09(result,lis_beta_ef_h, lis_tag_last, lis_end, lis_end_2low)\n",
    "#         lis_w, lis_t, lis_i, w_last, t_last, off_word, lis_normal, lis_target_ind = prepro_after(result, lis_end_2low)\n",
    "#         space_location = convertSpace(space_list, pre_w)\n",
    "        space_location = convertSpace(space_list, ori_w)\n",
    "#         lis_target_final = []\n",
    "        if len(target_ind)!=0:\n",
    "\n",
    "            jam = Jamodealer(ori_w)\n",
    "\n",
    "            for i in range(len(target_ind)):\n",
    "                #new_end = self.make_end_low(converted_w[target_ind[i]], last_ef[i], converted_t[target_ind[i]], kconverted_w[ktarget_ind[i]], klast_ef[i], kconverted_t[ktarget_ind[i]])\n",
    "                new_end = self.make_end_low(converted_w[target_ind[i]], last_ef[i], converted_t[target_ind[i]])\n",
    "                jam.jamo[target_ind[i]] = new_end\n",
    "            \n",
    "            \n",
    "#             jam.jamo.append(w_last)\n",
    "            union(space_location, jam.jamo)\n",
    "            return jam.make_one()\n",
    "\n",
    "        return input\n",
    "\n",
    "    def processText(self,stc):\n",
    "        result = stc\n",
    "        \n",
    "        flag = 0\n",
    "        if result[-1]=='\\n':\n",
    "            result = result.replace('\\n','')   \n",
    "        num = 0\n",
    "        while 1:\n",
    "            if result[-1-num]!=' ':\n",
    "                break\n",
    "            else:\n",
    "                num = num+1\n",
    "                \n",
    "        if num==0:\n",
    "            rere = result\n",
    "        else:\n",
    "            rere = result[:-num]\n",
    "            \n",
    "        \n",
    "        r_pun = ''\n",
    "        r_word = rere\n",
    "        while True:\n",
    "            if r_word[-1] in SV_LIST:\n",
    "                r_pun = r_pun+r_word[-1]\n",
    "                r_word = r_word[:-1]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        num_space = 0\n",
    "        for i in r_word:\n",
    "            if i==' ':\n",
    "                num_space = num_space+1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        if num_space!=0:\n",
    "            r_word = r_word[num_space:]\n",
    "\n",
    "        plus = ''\n",
    "        for s in range(num_space):\n",
    "            plus = plus+' '\n",
    "    \n",
    "        if r_word[-1] =='?' or r_word[-1] =='.' or r_word[-1] =='!' or r_word[-1] =='\\\"':\n",
    "            r_word = r_word\n",
    "        else:\n",
    "            r_word = r_word+'.'\n",
    "            flag = 1\n",
    "        try:\n",
    "            res = self.to_low(r_word)\n",
    "        except:\n",
    "            res = r_word\n",
    "        \n",
    "        r_word = plus+r_word\n",
    "        res = plus+res\n",
    "        \n",
    "        if flag ==1:\n",
    "            res = res[:-1]\n",
    "\n",
    "        return res+r_pun[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0819219",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"러시아가 우크라이나 동부에 군대를 투입하며 양국간 전쟁 위험이 고조되는 가운데 국방부는 “요청이 오면 재외국민 이송을 위해 적극 협조할 예정”이라고 밝혔습니다.\\\n",
    "22일 국방부는 “최근 우크라이나 사태에 대한 향후 상황 전개 과정을 면밀하게 주시하면서 관련 기관 간 긴밀한 공조체계를 유지하고 있다”면서 “재외국민 이송작전도 준비중”이라고 밝혔습니다.\\\n",
    "외교부에 따르면 우크라이나 동부 분쟁지역인 돈바스 지역에서 포격 공방에 발생함에 따라, 주우크라이나 한국 대사관은 지난 19일 현지 체류 중인 한국 국민들에게 조속히 대피, 철수할 것을 긴급 공지했습니다. 19일(현지 시간) 오후 6시 기준 파악된 우크라이나 체류 국민은 선교사 14명, 유학생 5명, 자영업자와 영주권자 등 49명입니다.\\\n",
    "현재 교민 40여명은 육로를 통해 현지를 벗어날 계획으로 알려졌지만 상황이 여의치 않을 경우에는 수송기를 급파할 수 밖에 없습니다. 러시아가 우크라이나를 침공하는 등 상황이 급변할 경우 지난해 8월 아프가니스탄 조력자 이송 작전(미라클 작전)과 유사한 작전이 펼쳐질 가능성이 있다는 것입니다.\\\n",
    "지난해 8월 군은 공군 수송기인 C-130 2대와 KC-330 1대를 아프가니스탄과 파키스탄으로 파견했습니다. 수송기는 한국에 협조했던 아프간 주민 380여명을 구출해 한국으로 이송했습니다.\\\n",
    "\"\n",
    "kori_w, kori_t, kind, kconverted_w, kconverted_t, klast_ef,ktarget_ind  = prepro_beta_khaiii(txt,lis_beta_ef_h, lis_tag_last, lis_end, lis_end_2low)\n",
    "ori_w, ori_t, ind, converted_w, converted_t, last_ef,target_ind  = prepro_beta_09(txt, lis_beta_ef_h, lis_tag_last, lis_end, lis_end_2low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6de6fcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['러시아가',\n",
       "  '우크라이나',\n",
       "  '동부에',\n",
       "  '군대를',\n",
       "  '투입하며',\n",
       "  '양국간',\n",
       "  '전쟁',\n",
       "  '위험이',\n",
       "  '고조되는',\n",
       "  '가운데',\n",
       "  '국방부는',\n",
       "  '“요청이',\n",
       "  '오면',\n",
       "  '재외국민',\n",
       "  '이송을',\n",
       "  '위해',\n",
       "  '적극',\n",
       "  '협조할',\n",
       "  '예정”이라고',\n",
       "  '밝혔습니다.22일',\n",
       "  '국방부는',\n",
       "  '“최근',\n",
       "  '우크라이나',\n",
       "  '사태에',\n",
       "  '대한',\n",
       "  '향후',\n",
       "  '상황',\n",
       "  '전개',\n",
       "  '과정을',\n",
       "  '면밀하게',\n",
       "  '주시하면서',\n",
       "  '관련',\n",
       "  '기관',\n",
       "  '간',\n",
       "  '긴밀한',\n",
       "  '공조체계를',\n",
       "  '유지하고',\n",
       "  '있다”면서',\n",
       "  '“재외국민',\n",
       "  '이송작전도',\n",
       "  '준비중”이라고',\n",
       "  '밝혔습니다.외교부에',\n",
       "  '따르면',\n",
       "  '우크라이나',\n",
       "  '동부',\n",
       "  '분쟁지역인',\n",
       "  '돈바스',\n",
       "  '지역에서',\n",
       "  '포격',\n",
       "  '공방에',\n",
       "  '발생함에',\n",
       "  '따라,',\n",
       "  '주우크라이나',\n",
       "  '한국',\n",
       "  '대사관은',\n",
       "  '지난',\n",
       "  '19일',\n",
       "  '현지',\n",
       "  '체류',\n",
       "  '중인',\n",
       "  '한국',\n",
       "  '국민들에게',\n",
       "  '조속히',\n",
       "  '대피,',\n",
       "  '철수할',\n",
       "  '것을',\n",
       "  '긴급',\n",
       "  '공지했습니다.',\n",
       "  '19일(현지',\n",
       "  '시간)',\n",
       "  '오후',\n",
       "  '6시',\n",
       "  '기준',\n",
       "  '파악된',\n",
       "  '우크라이나',\n",
       "  '체류',\n",
       "  '국민은',\n",
       "  '선교사',\n",
       "  '14명,',\n",
       "  '유학생',\n",
       "  '5명,',\n",
       "  '자영업자와',\n",
       "  '영주권자',\n",
       "  '등',\n",
       "  '49명입니다.현재',\n",
       "  '교민',\n",
       "  '40여명은',\n",
       "  '육로를',\n",
       "  '통해',\n",
       "  '현지를',\n",
       "  '벗어날',\n",
       "  '계획으로',\n",
       "  '알려졌지만',\n",
       "  '상황이',\n",
       "  '여의치',\n",
       "  '않을',\n",
       "  '경우에는',\n",
       "  '수송기를',\n",
       "  '급파할',\n",
       "  '수',\n",
       "  '밖에',\n",
       "  '없습니다.',\n",
       "  '러시아가',\n",
       "  '우크라이나를',\n",
       "  '침공하는',\n",
       "  '등',\n",
       "  '상황이',\n",
       "  '급변할',\n",
       "  '경우',\n",
       "  '지난해',\n",
       "  '8월',\n",
       "  '아프가니스탄',\n",
       "  '조력자',\n",
       "  '이송',\n",
       "  '작전(미라클',\n",
       "  '작전)과',\n",
       "  '유사한',\n",
       "  '작전이',\n",
       "  '펼쳐질',\n",
       "  '가능성이',\n",
       "  '있다는',\n",
       "  '것입니다.지난해',\n",
       "  '8월',\n",
       "  '군은',\n",
       "  '공군',\n",
       "  '수송기인',\n",
       "  'C-130',\n",
       "  '2대와',\n",
       "  'KC-330',\n",
       "  '1대를',\n",
       "  '아프가니스탄과',\n",
       "  '파키스탄으로',\n",
       "  '파견했습니다.',\n",
       "  '수송기는',\n",
       "  '한국에',\n",
       "  '협조했던',\n",
       "  '아프간',\n",
       "  '주민',\n",
       "  '380여명을',\n",
       "  '구출해',\n",
       "  '한국으로',\n",
       "  '이송했습니다.'],\n",
       " ['ㄹㅓㅅㅣㅇㅏㄱㅏ',\n",
       "  'ㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㄷㅗㅇㅂㅜㅇㅔ',\n",
       "  'ㄱㅜㄴㄷㅐㄹㅡㄹ',\n",
       "  'ㅌㅜㅇㅣㅂㅎㅏㅁㅕ',\n",
       "  'ㅇㅑㅇㄱㅜㄱㄱㅏㄴ',\n",
       "  'ㅈㅓㄴㅈㅐㅇ',\n",
       "  'ㅇㅟㅎㅓㅁㅇㅣ',\n",
       "  'ㄱㅗㅈㅗㄷㅚㄴㅡㄴ',\n",
       "  'ㄱㅏㅇㅜㄴㄷㅔ',\n",
       "  'ㄱㅜㄱㅂㅏㅇㅂㅜㄴㅡㄴ',\n",
       "  '“ㅇㅛㅊㅓㅇㅇㅣ',\n",
       "  'ㅇㅗㅁㅕㄴ',\n",
       "  'ㅈㅐㅇㅚㄱㅜㄱㅁㅣㄴ',\n",
       "  'ㅇㅣㅅㅗㅇㅇㅡㄹ',\n",
       "  'ㅇㅟㅎㅐ',\n",
       "  'ㅈㅓㄱㄱㅡㄱ',\n",
       "  'ㅎㅕㅂㅈㅗㅎㅏㄹ',\n",
       "  'ㅇㅖㅈㅓㅇ”ㅇㅣㄹㅏㄱㅗ',\n",
       "  'ㅂㅏㄺㅎㅕㅆ.22ㅇㅣㄹ',\n",
       "  'ㄱㅜㄱㅂㅏㅇㅂㅜㄴㅡㄴ',\n",
       "  '“ㅊㅚㄱㅡㄴ',\n",
       "  'ㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㅅㅏㅌㅐㅇㅔ',\n",
       "  'ㄷㅐㅎㅏㄴ',\n",
       "  'ㅎㅑㅇㅎㅜ',\n",
       "  'ㅅㅏㅇㅎㅘㅇ',\n",
       "  'ㅈㅓㄴㄱㅐ',\n",
       "  'ㄱㅘㅈㅓㅇㅇㅡㄹ',\n",
       "  'ㅁㅕㄴㅁㅣㄹㅎㅏㄱㅔ',\n",
       "  'ㅈㅜㅅㅣㅎㅏㅁㅕㄴㅅㅓ',\n",
       "  'ㄱㅘㄴㄹㅕㄴ',\n",
       "  'ㄱㅣㄱㅘㄴ',\n",
       "  'ㄱㅏㄴ',\n",
       "  'ㄱㅣㄴㅁㅣㄹㅎㅏㄴ',\n",
       "  'ㄱㅗㅇㅈㅗㅊㅔㄱㅖㄹㅡㄹ',\n",
       "  'ㅇㅠㅈㅣㅎㅏㄱㅗ',\n",
       "  'ㅇㅣㅆㄷㅏ”ㅁㅕㄴㅅㅓ',\n",
       "  '“ㅈㅐㅇㅚㄱㅜㄱㅁㅣㄴ',\n",
       "  'ㅇㅣㅅㅗㅇㅈㅏㄱㅈㅓㄴㄷㅗ',\n",
       "  'ㅈㅜㄴㅂㅣㅈㅜㅇ”ㅇㅣㄹㅏㄱㅗ',\n",
       "  'ㅂㅏㄺㅎㅕㅆ.ㅇㅚㄱㅛㅂㅜㅇㅔ',\n",
       "  'ㄸㅏㄹㅡㅁㅕㄴ',\n",
       "  'ㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㄷㅗㅇㅂㅜ',\n",
       "  'ㅂㅜㄴㅈㅐㅇㅈㅣㅇㅕㄱㅇㅣㄴ',\n",
       "  'ㄷㅗㄴㅂㅏㅅㅡ',\n",
       "  'ㅈㅣㅇㅕㄱㅇㅔㅅㅓ',\n",
       "  'ㅍㅗㄱㅕㄱ',\n",
       "  'ㄱㅗㅇㅂㅏㅇㅇㅔ',\n",
       "  'ㅂㅏㄹㅅㅐㅇㅎㅏㅁㅇㅔ',\n",
       "  'ㄸㅏㄹㅏ,',\n",
       "  'ㅈㅜㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㅎㅏㄴㄱㅜㄱ',\n",
       "  'ㄷㅐㅅㅏㄱㅘㄴㅇㅡㄴ',\n",
       "  'ㅈㅣㄴㅏㄴ',\n",
       "  '19ㅇㅣㄹ',\n",
       "  'ㅎㅕㄴㅈㅣ',\n",
       "  'ㅊㅔㄹㅠ',\n",
       "  'ㅈㅜㅇㅇㅣㄴ',\n",
       "  'ㅎㅏㄴㄱㅜㄱ',\n",
       "  'ㄱㅜㄱㅁㅣㄴㄷㅡㄹㅇㅔㄱㅔ',\n",
       "  'ㅈㅗㅅㅗㄱㅎㅣ',\n",
       "  'ㄷㅐㅍㅣ,',\n",
       "  'ㅊㅓㄹㅅㅜㅎㅏㄹ',\n",
       "  'ㄱㅓㅅㅇㅡㄹ',\n",
       "  'ㄱㅣㄴㄱㅡㅂ',\n",
       "  'ㄱㅗㅇㅈㅣㅎㅐㅆ.',\n",
       "  '19ㅇㅣㄹ(ㅎㅕㄴㅈㅣ',\n",
       "  'ㅅㅣㄱㅏㄴ)',\n",
       "  'ㅇㅗㅎㅜ',\n",
       "  '6ㅅㅣ',\n",
       "  'ㄱㅣㅈㅜㄴ',\n",
       "  'ㅍㅏㅇㅏㄱㄷㅚㄴ',\n",
       "  'ㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㅊㅔㄹㅠ',\n",
       "  'ㄱㅜㄱㅁㅣㄴㅇㅡㄴ',\n",
       "  'ㅅㅓㄴㄱㅛㅅㅏ',\n",
       "  '14ㅁㅕㅇ,',\n",
       "  'ㅇㅠㅎㅏㄱㅅㅐㅇ',\n",
       "  '5ㅁㅕㅇ,',\n",
       "  'ㅈㅏㅇㅕㅇㅇㅓㅂㅈㅏㅇㅘ',\n",
       "  'ㅇㅕㅇㅈㅜㄱㅝㄴㅈㅏ',\n",
       "  'ㄷㅡㅇ',\n",
       "  '49ㅁㅕㅇㅇㅣ.ㅎㅕㄴㅈㅐ',\n",
       "  'ㄱㅛㅁㅣㄴ',\n",
       "  '40ㅇㅕㅁㅕㅇㅇㅡㄴ',\n",
       "  'ㅇㅠㄱㄹㅗㄹㅡㄹ',\n",
       "  'ㅌㅗㅇㅎㅐ',\n",
       "  'ㅎㅕㄴㅈㅣㄹㅡㄹ',\n",
       "  'ㅂㅓㅅㅇㅓㄴㅏㄹ',\n",
       "  'ㄱㅖㅎㅚㄱㅇㅡㄹㅗ',\n",
       "  'ㅇㅏㄹㄹㅕㅈㅕㅆㅈㅣㅁㅏㄴ',\n",
       "  'ㅅㅏㅇㅎㅘㅇㅇㅣ',\n",
       "  'ㅇㅕㅇㅢㅊㅣ',\n",
       "  'ㅇㅏㄶㅇㅡㄹ',\n",
       "  'ㄱㅕㅇㅇㅜㅇㅔㄴㅡㄴ',\n",
       "  'ㅅㅜㅅㅗㅇㄱㅣㄹㅡㄹ',\n",
       "  'ㄱㅡㅂㅍㅏㅎㅏㄹ',\n",
       "  'ㅅㅜ',\n",
       "  'ㅂㅏㄲㅇㅔ',\n",
       "  'ㅇㅓㅄ.',\n",
       "  'ㄹㅓㅅㅣㅇㅏㄱㅏ',\n",
       "  'ㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏㄹㅡㄹ',\n",
       "  'ㅊㅣㅁㄱㅗㅇㅎㅏㄴㅡㄴ',\n",
       "  'ㄷㅡㅇ',\n",
       "  'ㅅㅏㅇㅎㅘㅇㅇㅣ',\n",
       "  'ㄱㅡㅂㅂㅕㄴㅎㅏㄹ',\n",
       "  'ㄱㅕㅇㅇㅜ',\n",
       "  'ㅈㅣㄴㅏㄴㅎㅐ',\n",
       "  '8ㅇㅝㄹ',\n",
       "  'ㅇㅏㅍㅡㄱㅏㄴㅣㅅㅡㅌㅏㄴ',\n",
       "  'ㅈㅗㄹㅕㄱㅈㅏ',\n",
       "  'ㅇㅣㅅㅗㅇ',\n",
       "  'ㅈㅏㄱㅈㅓㄴ(ㅁㅣㄹㅏㅋㅡㄹ',\n",
       "  'ㅈㅏㄱㅈㅓㄴ)ㄱㅘ',\n",
       "  'ㅇㅠㅅㅏㅎㅏㄴ',\n",
       "  'ㅈㅏㄱㅈㅓㄴㅇㅣ',\n",
       "  'ㅍㅕㄹㅊㅕㅈㅣㄹ',\n",
       "  'ㄱㅏㄴㅡㅇㅅㅓㅇㅇㅣ',\n",
       "  'ㅇㅣㅆㄷㅏㄴㅡㄴ',\n",
       "  'ㄱㅓㅅㅇㅣ.ㅈㅣㄴㅏㄴㅎㅐ',\n",
       "  '8ㅇㅝㄹ',\n",
       "  'ㄱㅜㄴㅇㅡㄴ',\n",
       "  'ㄱㅗㅇㄱㅜㄴ',\n",
       "  'ㅅㅜㅅㅗㅇㄱㅣㅇㅣㄴ',\n",
       "  'C-130',\n",
       "  '2ㄷㅐㅇㅘ',\n",
       "  'KC-330',\n",
       "  '1ㄷㅐㄹㅡㄹ',\n",
       "  'ㅇㅏㅍㅡㄱㅏㄴㅣㅅㅡㅌㅏㄴㄱㅘ',\n",
       "  'ㅍㅏㅋㅣㅅㅡㅌㅏㄴㅇㅡㄹㅗ',\n",
       "  'ㅍㅏㄱㅕㄴㅎㅐㅆ.',\n",
       "  'ㅅㅜㅅㅗㅇㄱㅣㄴㅡㄴ',\n",
       "  'ㅎㅏㄴㄱㅜㄱㅇㅔ',\n",
       "  'ㅎㅕㅂㅈㅗㅎㅐㅆㄷㅓㄴ',\n",
       "  'ㅇㅏㅍㅡㄱㅏㄴ',\n",
       "  'ㅈㅜㅁㅣㄴ',\n",
       "  '380ㅇㅕㅁㅕㅇㅇㅡㄹ',\n",
       "  'ㄱㅜㅊㅜㄹㅎㅐ',\n",
       "  'ㅎㅏㄴㄱㅜㄱㅇㅡㄹㅗ',\n",
       "  'ㅇㅣㅅㅗㅇㅎㅐㅆ.'],\n",
       " ['NNP/JKS',\n",
       "  'NNP/JC',\n",
       "  'NNG/JKB',\n",
       "  'NNG/JKO',\n",
       "  'NNG/XSV/EC',\n",
       "  'NNG/NNB',\n",
       "  'NNG',\n",
       "  'NNG/JKS',\n",
       "  'NNG/XSV/ETM',\n",
       "  'NNG',\n",
       "  'NNG/JX',\n",
       "  'SSO/NNG/JKS',\n",
       "  'VV/EC',\n",
       "  'NNG/NNG',\n",
       "  'NNG/JKO',\n",
       "  'VV+EC',\n",
       "  'NNG',\n",
       "  'NNG/XSV+ETM',\n",
       "  'NNG/SSC/VCP/EC',\n",
       "  'VV+EP/NULL/SF/SN/NNBC',\n",
       "  'NNG/JX',\n",
       "  'SSO/NNG',\n",
       "  'NNP/JC',\n",
       "  'NNG/JKB',\n",
       "  'VV+ETM',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'NNG/JKO',\n",
       "  'XR/XSA/EC',\n",
       "  'VX/EP/XSA/EC',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'NNB',\n",
       "  'XR/XSA+ETM',\n",
       "  'NNG/NNG/JKO',\n",
       "  'NNG/XSV/EC',\n",
       "  'VX/EC/SSC/EC',\n",
       "  'SSO/NNG/NNG',\n",
       "  'NNG/NNG/JX',\n",
       "  'VX+ETM/NNG/SSC/VCP/EC',\n",
       "  'VV+EP/NULL/SF/NNP/JKB',\n",
       "  'VV/EC',\n",
       "  'NNP/JC',\n",
       "  'NNG',\n",
       "  'NNG/NNG/VCP+ETM',\n",
       "  'NNP',\n",
       "  'NNG/JKB',\n",
       "  'NNG',\n",
       "  'NNG/JKB',\n",
       "  'NNG/XSV+ETN/JKB',\n",
       "  'VV+EC/SC',\n",
       "  'NNP/NNP/JC',\n",
       "  'NNP',\n",
       "  'NNG/JX',\n",
       "  'VV+ETM',\n",
       "  'SN/NNBC',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'NNB/VCP+ETM',\n",
       "  'NNP',\n",
       "  'NNG/XSN/JKB',\n",
       "  'MAG',\n",
       "  'NNG/SC',\n",
       "  'NNG/XSV+ETM',\n",
       "  'NNB/JKO',\n",
       "  'NNG',\n",
       "  'NNG/XSV+EP/NULL/SF',\n",
       "  'SN/NNBC/SSO/NNG',\n",
       "  'NNG/SSC',\n",
       "  'NNG',\n",
       "  'SN/NNBC',\n",
       "  'NNG',\n",
       "  'NNG/XSV+ETM',\n",
       "  'NNP/JC',\n",
       "  'NNG',\n",
       "  'NNG/JX',\n",
       "  'NNG',\n",
       "  'SN/NNBC/SC',\n",
       "  'NNG',\n",
       "  'SN/NNBC/SC',\n",
       "  'IC/NNG/JC',\n",
       "  'NNG/XSN/XSN',\n",
       "  'NNB',\n",
       "  'SN/NNBC/VCP/SF/MAG',\n",
       "  'NNG',\n",
       "  'SN/XSN/NNBC/JX',\n",
       "  'NNG/JKO',\n",
       "  'VV+EC',\n",
       "  'NNG/JKO',\n",
       "  'VV+ETM',\n",
       "  'NNG/JKB',\n",
       "  'VV+EC+VX+EP/EC',\n",
       "  'NNG/JKS',\n",
       "  'VA+EC',\n",
       "  'VX/ETM',\n",
       "  'NNG/JKB/JX',\n",
       "  'NNG/JKO',\n",
       "  'NNG/XSV+ETM',\n",
       "  'NNB',\n",
       "  'NNG/JKB',\n",
       "  'VA/NULL/SF',\n",
       "  'NNP/JKS',\n",
       "  'NNP/JKO',\n",
       "  'NNG/XSV/ETM',\n",
       "  'NNB',\n",
       "  'NNG/JKS',\n",
       "  'NNG/XSV+ETM',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'SN/NNBC',\n",
       "  'NNP',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'NNG/SSO/NNP',\n",
       "  'NNG/SSC/JC',\n",
       "  'NNG/XSA+ETM',\n",
       "  'NNG/JKS',\n",
       "  'VV+ETM',\n",
       "  'NNG/XSN/JKS',\n",
       "  'VV/ETM',\n",
       "  'NNB/VCP/SF/NNG',\n",
       "  'SN/NNBC',\n",
       "  'NNB/JX',\n",
       "  'NNG',\n",
       "  'NNG/VCP+ETM',\n",
       "  'SL/SY/SN',\n",
       "  'SN/NNBC/JC',\n",
       "  'SL/SY/SN',\n",
       "  'SN/NNBC/JKO',\n",
       "  'NNP/JC',\n",
       "  'NNP/JKB',\n",
       "  'NNG/XSV+EP/NULL/SF',\n",
       "  'NNG/JX',\n",
       "  'NNP/JKB',\n",
       "  'NNG/XSV+EP/ETM',\n",
       "  'NNG',\n",
       "  'NNG',\n",
       "  'SN/XSN/NNBC/JKO',\n",
       "  'NNG/XSV+EC',\n",
       "  'NNP/JKB',\n",
       "  'NNG/XSV+EP/NULL/SF'],\n",
       " ['ㅅㅡㅂㄴㅣㄷㅏ',\n",
       "  'ㅅㅡㅂㄴㅣㄷㅏ',\n",
       "  'ㅅㅡㅂㄴㅣㄷㅏ',\n",
       "  'ㅂㄴㅣㄷㅏ',\n",
       "  'ㅅㅡㅂㄴㅣㄷㅏ',\n",
       "  'ㅂㄴㅣㄷㅏ',\n",
       "  'ㅅㅡㅂㄴㅣㄷㅏ',\n",
       "  'ㅅㅡㅂㄴㅣㄷㅏ'],\n",
       " [19, 41, 67, 84, 101, 121, 132, 141])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_w, converted_w,converted_t, last_ef, target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9b4f16e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['러시아가',\n",
       "  '우크라이나',\n",
       "  '동부에',\n",
       "  '군대를',\n",
       "  '투입하며',\n",
       "  '양국간',\n",
       "  '전쟁',\n",
       "  '위험이',\n",
       "  '고조되는',\n",
       "  '가운데',\n",
       "  '국방부는',\n",
       "  '“요청이',\n",
       "  '오면',\n",
       "  '재외국민',\n",
       "  '이송을',\n",
       "  '위하',\n",
       "  '여적극협조',\n",
       "  '하ㄹ예정”이라고밝히었습니다.',\n",
       "  '22일국방부는',\n",
       "  '“최근우크라이나',\n",
       "  '사태',\n",
       "  '에대하ㄴ',\n",
       "  '향후',\n",
       "  '상황전개과정을',\n",
       "  '면밀하게주시하',\n",
       "  '면서',\n",
       "  '관련기관',\n",
       "  '간긴밀하ㄴ',\n",
       "  '공조체계',\n",
       "  '를유지하고',\n",
       "  '있다”면서',\n",
       "  '“재외국민',\n",
       "  '이송작전도준비중”이라고밝히었습니다.외교부에따르',\n",
       "  '면우크라이나동부분쟁',\n",
       "  '지역이ㄴ',\n",
       "  '돈바스지역',\n",
       "  '에서포격',\n",
       "  '공방에',\n",
       "  '발생하ㅁ에따르아',\n",
       "  ',주우크라이나한국',\n",
       "  '대사관은',\n",
       "  '지나',\n",
       "  'ㄴ19일현지체류중이',\n",
       "  'ㄴ한국',\n",
       "  '국민들',\n",
       "  '에게',\n",
       "  '조속히대피,철수하',\n",
       "  'ㄹ것을긴급공지하였',\n",
       "  '습니다.',\n",
       "  '19',\n",
       "  '일(현지시간)오후6',\n",
       "  '시기준',\n",
       "  '파악되',\n",
       "  'ㄴ우크라이나체류국민',\n",
       "  '은선교사14명,유학',\n",
       "  '생5명,자영업자와',\n",
       "  '영주',\n",
       "  '권자등49',\n",
       "  '명이ㅂ니다',\n",
       "  '.현재',\n",
       "  '교민40여명은',\n",
       "  '육로를통하',\n",
       "  '여현지',\n",
       "  '를벗어나ㄹ',\n",
       "  '계획으로',\n",
       "  '알리어지',\n",
       "  '었지만',\n",
       "  '상황이',\n",
       "  '여의하지않',\n",
       "  '을경우에',\n",
       "  '는수송기를급파하ㄹ수',\n",
       "  '밖',\n",
       "  '에없습니다.',\n",
       "  '러시아가우크라이나를침공하',\n",
       "  '는등상황이',\n",
       "  '급변하ㄹ경우',\n",
       "  '지난해8',\n",
       "  '월아프가니스탄조력자이송작전(미',\n",
       "  '라클작전)과유사',\n",
       "  '하ㄴ',\n",
       "  '작전',\n",
       "  '이펼쳐지ㄹ가능성이있다는것이',\n",
       "  'ㅂ니다.지난해8월',\n",
       "  '군은공군수송기',\n",
       "  '이ㄴC-1302대와KC-',\n",
       "  '3301',\n",
       "  '대를아프가니스탄과파키스탄으로파견하',\n",
       "  '였습니다.수송기는한국에협조하였던아프간주민380여명을구출하여한국으로이송하였습니다.'],\n",
       " ['ㄹㅓㅅㅣㅇㅏㄱㅏ',\n",
       "  'ㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㄷㅗㅇㅂㅜㅇㅔ',\n",
       "  'ㄱㅜㄴㄷㅐㄹㅡㄹ',\n",
       "  'ㅌㅜㅇㅣㅂㅎㅏㅁㅕ',\n",
       "  'ㅇㅑㅇㄱㅜㄱㄱㅏㄴ',\n",
       "  'ㅈㅓㄴㅈㅐㅇ',\n",
       "  'ㅇㅟㅎㅓㅁㅇㅣ',\n",
       "  'ㄱㅗㅈㅗㄷㅚㄴㅡㄴ',\n",
       "  'ㄱㅏㅇㅜㄴㄷㅔ',\n",
       "  'ㄱㅜㄱㅂㅏㅇㅂㅜㄴㅡㄴ',\n",
       "  '“ㅇㅛㅊㅓㅇㅇㅣ',\n",
       "  'ㅇㅗㅁㅕㄴ',\n",
       "  'ㅈㅐㅇㅚㄱㅜㄱㅁㅣㄴ',\n",
       "  'ㅇㅣㅅㅗㅇㅇㅡㄹ',\n",
       "  'ㅇㅟㅎㅏ',\n",
       "  'ㅇㅕㅈㅓㄱㄱㅡㄱㅎㅕㅂㅈㅗ',\n",
       "  'ㅎㅏㄹㅇㅖㅈㅓㅇ”ㅇㅣㄹㅏㄱㅗㅂㅏㄺㅎㅣㅇㅓㅆ.',\n",
       "  '22ㅇㅣㄹㄱㅜㄱㅂㅏㅇㅂㅜㄴㅡㄴ',\n",
       "  '“ㅊㅚㄱㅡㄴㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏ',\n",
       "  'ㅅㅏㅌㅐ',\n",
       "  'ㅇㅔㄷㅐㅎㅏㄴ',\n",
       "  'ㅎㅑㅇㅎㅜ',\n",
       "  'ㅅㅏㅇㅎㅘㅇㅈㅓㄴㄱㅐㄱㅘㅈㅓㅇㅇㅡㄹ',\n",
       "  'ㅁㅕㄴㅁㅣㄹㅎㅏㄱㅔㅈㅜㅅㅣㅎㅏ',\n",
       "  'ㅁㅕㄴㅅㅓ',\n",
       "  'ㄱㅘㄴㄹㅕㄴㄱㅣㄱㅘㄴ',\n",
       "  'ㄱㅏㄴㄱㅣㄴㅁㅣㄹㅎㅏㄴ',\n",
       "  'ㄱㅗㅇㅈㅗㅊㅔㄱㅖ',\n",
       "  'ㄹㅡㄹㅇㅠㅈㅣㅎㅏㄱㅗ',\n",
       "  'ㅇㅣㅆㄷㅏ”ㅁㅕㄴㅅㅓ',\n",
       "  '“ㅈㅐㅇㅚㄱㅜㄱㅁㅣㄴ',\n",
       "  'ㅇㅣㅅㅗㅇㅈㅏㄱㅈㅓㄴㄷㅗㅈㅜㄴㅂㅣㅈㅜㅇ”ㅇㅣㄹㅏㄱㅗㅂㅏㄺㅎㅣㅇㅓㅆ.ㅇㅚㄱㅛㅂㅜㅇㅔㄸㅏㄹㅡ',\n",
       "  'ㅁㅕㄴㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏㄷㅗㅇㅂㅜㅂㅜㄴㅈㅐㅇ',\n",
       "  'ㅈㅣㅇㅕㄱㅇㅣㄴ',\n",
       "  'ㄷㅗㄴㅂㅏㅅㅡㅈㅣㅇㅕㄱ',\n",
       "  'ㅇㅔㅅㅓㅍㅗㄱㅕㄱ',\n",
       "  'ㄱㅗㅇㅂㅏㅇㅇㅔ',\n",
       "  'ㅂㅏㄹㅅㅐㅇㅎㅏㅁㅇㅔㄸㅏㄹㅡㅇㅏ',\n",
       "  ',ㅈㅜㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏㅎㅏㄴㄱㅜㄱ',\n",
       "  'ㄷㅐㅅㅏㄱㅘㄴㅇㅡㄴ',\n",
       "  'ㅈㅣㄴㅏ',\n",
       "  'ㄴ19ㅇㅣㄹㅎㅕㄴㅈㅣㅊㅔㄹㅠㅈㅜㅇㅇㅣ',\n",
       "  'ㄴㅎㅏㄴㄱㅜㄱ',\n",
       "  'ㄱㅜㄱㅁㅣㄴㄷㅡㄹ',\n",
       "  'ㅇㅔㄱㅔ',\n",
       "  'ㅈㅗㅅㅗㄱㅎㅣㄷㅐㅍㅣ,ㅊㅓㄹㅅㅜㅎㅏ',\n",
       "  'ㄹㄱㅓㅅㅇㅡㄹㄱㅣㄴㄱㅡㅂㄱㅗㅇㅈㅣㅎㅏㅇㅕㅆ',\n",
       "  '.',\n",
       "  '19',\n",
       "  'ㅇㅣㄹ(ㅎㅕㄴㅈㅣㅅㅣㄱㅏㄴ)ㅇㅗㅎㅜ6',\n",
       "  'ㅅㅣㄱㅣㅈㅜㄴ',\n",
       "  'ㅍㅏㅇㅏㄱㄷㅚ',\n",
       "  'ㄴㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏㅊㅔㄹㅠㄱㅜㄱㅁㅣㄴ',\n",
       "  'ㅇㅡㄴㅅㅓㄴㄱㅛㅅㅏ14ㅁㅕㅇ,ㅇㅠㅎㅏㄱ',\n",
       "  'ㅅㅐㅇ5ㅁㅕㅇ,ㅈㅏㅇㅕㅇㅇㅓㅂㅈㅏㅇㅘ',\n",
       "  'ㅇㅕㅇㅈㅜ',\n",
       "  'ㄱㅝㄴㅈㅏㄷㅡㅇ49',\n",
       "  'ㅁㅕㅇㅇㅣㅂㄴㅣㄷㅏ',\n",
       "  '.ㅎㅕㄴㅈㅐ',\n",
       "  'ㄱㅛㅁㅣㄴ40ㅇㅕㅁㅕㅇㅇㅡㄴ',\n",
       "  'ㅇㅠㄱㄹㅗㄹㅡㄹㅌㅗㅇㅎㅏ',\n",
       "  'ㅇㅕㅎㅕㄴㅈㅣ',\n",
       "  'ㄹㅡㄹㅂㅓㅅㅇㅓㄴㅏㄹ',\n",
       "  'ㄱㅖㅎㅚㄱㅇㅡㄹㅗ',\n",
       "  'ㅇㅏㄹㄹㅣㅇㅓㅈㅣ',\n",
       "  'ㅇㅓㅆㅈㅣㅁㅏㄴ',\n",
       "  'ㅅㅏㅇㅎㅘㅇㅇㅣ',\n",
       "  'ㅇㅕㅇㅢㅎㅏㅈㅣㅇㅏㄶ',\n",
       "  'ㅇㅡㄹㄱㅕㅇㅇㅜㅇㅔ',\n",
       "  'ㄴㅡㄴㅅㅜㅅㅗㅇㄱㅣㄹㅡㄹㄱㅡㅂㅍㅏㅎㅏㄹㅅㅜ',\n",
       "  'ㅂㅏㄲ',\n",
       "  'ㅇㅔㅇㅓㅄ.',\n",
       "  'ㄹㅓㅅㅣㅇㅏㄱㅏㅇㅜㅋㅡㄹㅏㅇㅣㄴㅏㄹㅡㄹㅊㅣㅁㄱㅗㅇㅎㅏ',\n",
       "  'ㄴㅡㄴㄷㅡㅇㅅㅏㅇㅎㅘㅇㅇㅣ',\n",
       "  'ㄱㅡㅂㅂㅕㄴㅎㅏㄹㄱㅕㅇㅇㅜ',\n",
       "  'ㅈㅣㄴㅏㄴㅎㅐ8',\n",
       "  'ㅇㅝㄹㅇㅏㅍㅡㄱㅏㄴㅣㅅㅡㅌㅏㄴㅈㅗㄹㅕㄱㅈㅏㅇㅣㅅㅗㅇㅈㅏㄱㅈㅓㄴ(ㅁㅣ',\n",
       "  'ㄹㅏㅋㅡㄹㅈㅏㄱㅈㅓㄴ)ㄱㅘㅇㅠㅅㅏ',\n",
       "  'ㅎㅏㄴ',\n",
       "  'ㅈㅏㄱㅈㅓㄴ',\n",
       "  'ㅇㅣㅍㅕㄹㅊㅕㅈㅣㄹㄱㅏㄴㅡㅇㅅㅓㅇㅇㅣㅇㅣㅆㄷㅏㄴㅡㄴㄱㅓㅅㅇㅣ',\n",
       "  '.ㅈㅣㄴㅏㄴㅎㅐ8ㅇㅝㄹ',\n",
       "  'ㄱㅜㄴㅇㅡㄴㄱㅗㅇㄱㅜㄴㅅㅜㅅㅗㅇㄱㅣ',\n",
       "  'ㅇㅣㄴC-1302ㄷㅐㅇㅘKC-',\n",
       "  '3301',\n",
       "  'ㄷㅐㄹㅡㄹㅇㅏㅍㅡㄱㅏㄴㅣㅅㅡㅌㅏㄴㄱㅘㅍㅏㅋㅣㅅㅡㅌㅏㄴㅇㅡㄹㅗㅍㅏㄱㅕㄴㅎㅏ',\n",
       "  'ㅇㅕㅆ.ㅅㅜㅅㅗㅇㄱㅣㄴㅡㄴㅎㅏㄴㄱㅜㄱㅇㅔㅎㅕㅂㅈㅗㅎㅏㅇㅕㅆㄷㅓㄴㅇㅏㅍㅡㄱㅏㄴㅈㅜㅁㅣㄴ380ㅇㅕㅁㅕㅇㅇㅡㄹㄱㅜㅊㅜㄹㅎㅏㅇㅕㅎㅏㄴㄱㅜㄱㅇㅡㄹㅗㅇㅣㅅㅗㅇㅎㅏㅇㅕㅆ.'],\n",
       " ['NNP/JKS',\n",
       "  'NNP',\n",
       "  'NNG/JKB',\n",
       "  'NNG/JKO',\n",
       "  'NNG/XSV/EC',\n",
       "  'NNG/NNB',\n",
       "  'NNG',\n",
       "  'NNG/JKS',\n",
       "  'NNG/XSV/ETM',\n",
       "  'NNG',\n",
       "  'NNG/JX',\n",
       "  'SS/NNG/JKS',\n",
       "  'VV/EC',\n",
       "  'NNG/NNG',\n",
       "  'NNG/JKO',\n",
       "  'VV',\n",
       "  'EC/NNG/NNG',\n",
       "  'XSV/ETM/NNG/SS/JKQ/VV/EP/NULL/SF',\n",
       "  'SN/NNB/NNG/JX',\n",
       "  'SS/NNG/NNP',\n",
       "  'NNG',\n",
       "  'JKB/VV/ETM',\n",
       "  'NNG',\n",
       "  'NNG/NNG/NNG/JKO',\n",
       "  'XR/XSA/EC/NNG/XSV',\n",
       "  'EC',\n",
       "  'NNG/NNG',\n",
       "  'NNB/XR/XSA/ETM',\n",
       "  'NNG/NNG',\n",
       "  'JKO/NNG/XSV/EC',\n",
       "  'VX/EF/SS/EC',\n",
       "  'SS/NNG/NNG',\n",
       "  'NNG/NNG/JX/NNG/SS/VCP/JKQ/VV/EP/NULL/SF/NNG/JKB/VV',\n",
       "  'EC/NNP/NNG/NNG',\n",
       "  'NNG/VCP/ETM',\n",
       "  'NNP/NNG',\n",
       "  'JKB/NNG',\n",
       "  'NNG/JKB',\n",
       "  'NNG/XSV/ETN/JKB/VV/EC',\n",
       "  'SP/NNP/JC/NNP',\n",
       "  'NNG/JX',\n",
       "  'VV',\n",
       "  'ETM/SN/NNB/NNG/NNG/NNB/VCP',\n",
       "  'ETM/NNP',\n",
       "  'NNG/XSN',\n",
       "  'JKB',\n",
       "  'MAG/NNG/SP/NNG/XSV',\n",
       "  'ETM/NNB/JKO/NNG/NNG/XSV/EP',\n",
       "  'NULL/SF',\n",
       "  'SN',\n",
       "  'NNB/SS/NNG/NNG/SS/NNG/SN',\n",
       "  'NNB/NNG',\n",
       "  'NNG/XSV',\n",
       "  'ETM/NNP/NNG/NNG',\n",
       "  'JX/NNG/SN/NNB/SP/NNG',\n",
       "  'XSN/SN/NNB/SP/NNG/JC',\n",
       "  'NNG',\n",
       "  'XSN/NNG/NNB/SN',\n",
       "  'NNB/VCP/EF',\n",
       "  'SF/MAG',\n",
       "  'NNG/SN/XSN/NNB/JX',\n",
       "  'NNG/JKO/VV',\n",
       "  'EC/NNG',\n",
       "  'JKO/VV/ETM',\n",
       "  'NNG/JKB',\n",
       "  'VV/EC/VX',\n",
       "  'EP/EC',\n",
       "  'NNG/JKS',\n",
       "  'XR/XSA/EC/VX',\n",
       "  'ETM/NNG/JKB',\n",
       "  'JX/NNG/JKO/NNG/XSV/ETM/NNB',\n",
       "  'NNG',\n",
       "  'JKB/VA/NULL/SF',\n",
       "  'NNP/JKS/NNP/JKO/NNG/XSV',\n",
       "  'ETM/NNB/NNG/JKS',\n",
       "  'NNG/XSV/ETM/NNG',\n",
       "  'NNG/SN',\n",
       "  'NNB/NNP/NNG/NNG/NNG/SS/NNG',\n",
       "  'NNP/NNG/SS/JC/NNG',\n",
       "  'XSA/ETM',\n",
       "  'NNG',\n",
       "  'JKS/VV/ETM/NNG/XSN/JKS/VV/ETM/NNB/VCP',\n",
       "  'NULL/SF/XR/NNG/SN/NNB',\n",
       "  'NNB/JX/NNG/NNG',\n",
       "  'VCP/ETM/SL/SS/SN/SN/NNB/JC/SL/SS',\n",
       "  'SN/SN',\n",
       "  'NNB/JKO/NNP/JC/NNP/JKB/NNG/XSV',\n",
       "  'EP/NULL/SF/NNG/JX/NNP/JKB/NNG/XSV/EP/ETM/NNP/NNG/SN/XSN/NNB/JKO/NNG/XSV/EC/NNP/JKB/NNG/XSV/EP/NULL/SF'],\n",
       " ['ㅅㅡㅂㄴㅣㄷㅏ', 'ㅅㅡㅂㄴㅣㄷㅏ', 'ㅅㅡㅂㄴㅣㄷㅏ', 'ㅅㅡㅂㄴㅣㄷㅏ', 'ㅂㄴㅣㄷㅏ', 'ㅅㅡㅂㄴㅣㄷㅏ', 'ㅅㅡㅂㄴㅣㄷㅏ'],\n",
       " [17, 32, 48, 72, 82, 87, 87])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kori_w, kconverted_w,kconverted_t, klast_ef, ktarget_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "28af5536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'러시아가 우크라이나 동부에 군대를 투입하며 양국간 전쟁 위험이 고조되는 가운데 국방부는 “요청이 오면 재외국민 이송을 위해 적극 협조할 예정”이라고 밝혔다.22일 국방부는 “최근 우크라이나 사태에 대한 향후 상황 전개 과정을 면밀하게 주시하면서 관련 기관 간 긴밀한 공조체계를 유지하고 있다”면서 “재외국민 이송작전도 준비중”이라고 밝혔다.외교부에 따르면 우크라이나 동부 분쟁지역인 돈바스 지역에서 포격 공방에 발생함에 따라, 주우크라이나 한국 대사관은 지난 19일 현지 체류 중인 한국 국민들에게 조속히 대피, 철수할 것을 긴급 공지했다. 19일(현지 시간) 오후 6시 기준 파악된 우크라이나 체류 국민은 선교사 14명, 유학생 5명, 자영업자와 영주권자 등 49명이다.현재 교민 40여명은 육로를 통해 현지를 벗어날 계획으로 알려졌지만 상황이 여의치 않을 경우에는 수송기를 급파할 수 밖에 없다. 러시아가 우크라이나를 침공하는 등 상황이 급변할 경우 지난해 8월 아프가니스탄 조력자 이송 작전(미라클 작전)과 유사한 작전이 펼쳐질 가능성이 있다는 것이다.지난해 8월 군은 공군 수송기인 C-130 2대와 KC-330 1대를 아프가니스탄과 파키스탄으로 파견했다. 수송기는 한국에 협조했던 아프간 주민 380여명을 구출해 한국으로 이송했다.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = Changer()\n",
    "ch.processText(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d55c2987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ULL'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'VV+EC/SF'\n",
    "s = 'VV/NULL/SF'\n",
    "s[-6:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e2d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf13c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2534dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
